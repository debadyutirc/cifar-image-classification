{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import urllib\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "# Use Floyd's cifar-10 dataset if present\n",
    "floyd_cifar10_location = '/input/cifar-10/python.tar.gz'\n",
    "if isfile(floyd_cifar10_location):\n",
    "    tar_gz_path = floyd_cifar10_location\n",
    "else:\n",
    "    tar_gz_path = 'cifar-10-python.tar.gz'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(tar_gz_path):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            tar_gz_path,\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open(tar_gz_path) as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f05bcf0ba58>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_normal = np.zeros(tuple(x.shape))\n",
    "    x_max, x_min = x.max(), x.min()\n",
    "    no_images = x.shape[0]\n",
    "    for image_ix in range(no_images):\n",
    "        x_normal[image_ix, ...] = (x[image_ix, ...] - float(x_min)) / float(x_max - x_min)\n",
    "    return x_normal\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(n_values=10)\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    x_reshape = np.array(x).reshape(-1, 1)\n",
    "    return encoder.fit_transform(x_reshape).toarray()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=((None, ) + image_shape), name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, shape=(None, n_classes), name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    # defining weights and bias \n",
    "    weights = tf.Variable(tf.truncated_normal(list(conv_ksize) + [x_tensor.get_shape().as_list()[3],\n",
    "                                                                  conv_num_outputs], stddev=0.05))\n",
    "    \n",
    "    bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "    \n",
    "    # 2 D Convolution\n",
    "    output = tf.nn.conv2d(x_tensor, weights,\n",
    "                          strides=[1, conv_strides[0], conv_strides[1], 1],\n",
    "                          padding='SAME')\n",
    "    output = tf.nn.bias_add(output, bias)\n",
    "    output = tf.nn.relu(output)\n",
    "    \n",
    "    # Pooling\n",
    "    output = tf.nn.max_pool(output,\n",
    "                            ksize=[1, pool_ksize[0], pool_ksize[1], 1],\n",
    "                            strides=[1, pool_strides[0], pool_strides[1], 1],\n",
    "                            padding='SAME')\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    # capture flat dimension length\n",
    "    flat_shape = np.array(tensor_shape[1: ]).prod()\n",
    "    # modify batch size using tf.shape\n",
    "    return tf.reshape(x_tensor, [tf.shape(x_tensor)[0], flat_shape])\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    # capture flat dimension length\n",
    "    flat_shape = np.array(tensor_shape[1: ]).prod()   \n",
    "    # weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([flat_shape, num_outputs], stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    # fully connected layer\n",
    "    fc_layer = tf.nn.relu(tf.add(tf.matmul(x_tensor, weights), bias))\n",
    "    return fc_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    tensor_shape = x_tensor.get_shape().as_list()\n",
    "    # capture flat dimension length\n",
    "    flat_shape = np.array(tensor_shape[1: ]).prod()   \n",
    "    # weights and bias\n",
    "    weights = tf.Variable(tf.truncated_normal([flat_shape, num_outputs], stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    # fully connected layer\n",
    "    op_layer = tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "    return op_layer\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_ksize = (6, 6)\n",
    "    conv_num_outputs = 64\n",
    "    conv_strides = (1 , 1)\n",
    "    pool_ksize = (3, 3)\n",
    "    pool_strides = (2, 2)\n",
    "    num_outputs = 10\n",
    "    \n",
    "    conv_layer_1 = conv2d_maxpool(x, \n",
    "                                  conv_num_outputs, \n",
    "                                  conv_ksize, \n",
    "                                  conv_strides, \n",
    "                                  pool_ksize, \n",
    "                                  pool_strides)\n",
    "    \n",
    "    conv_layer_2 = conv2d_maxpool(x, \n",
    "                                  conv_num_outputs, \n",
    "                                  conv_ksize, \n",
    "                                  conv_strides, \n",
    "                                  pool_ksize, \n",
    "                                  pool_strides)\n",
    "    conv_layer_3 = conv2d_maxpool(x, \n",
    "                                  conv_num_outputs, \n",
    "                                  conv_ksize, \n",
    "                                  conv_strides, \n",
    "                                  pool_ksize, \n",
    "                                  pool_strides)\n",
    "\n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    \n",
    "    flatten_layer = flatten(conv_layer_3)\n",
    "    flatten_layer = tf.nn.dropout(flatten_layer, keep_prob)    \n",
    "\n",
    "    \n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    \n",
    "    fc_layer = fully_conn(flatten_layer, num_outputs*2)\n",
    "    fc_layer = tf.nn.dropout(fc_layer, keep_prob)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    op_layer = output(fc_layer, num_outputs)\n",
    "    \n",
    "    # TODO: return output\n",
    "    return op_layer\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch,\n",
    "                                      y: label_batch,\n",
    "                                      keep_prob: keep_probability})\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    \n",
    "    loss = session.run(cost, feed_dict={x: feature_batch,\n",
    "                                        y: label_batch,\n",
    "                                        keep_prob: 1.})\n",
    "    \n",
    "    accuracy = session.run(accuracy, feed_dict={x: feature_batch, \n",
    "                                                y: label_batch, \n",
    "                                                keep_prob: 1.})\n",
    "    \n",
    "    print('Loss: {:.4f} Validation Accuracy: {:.4f}'.format(loss, accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 100\n",
    "batch_size = 2048\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2546 Validation Accuracy: 14.7277\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.2030 Validation Accuracy: 20.6683\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.1348 Validation Accuracy: 25.4950\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.0814 Validation Accuracy: 27.3515\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 2.0319 Validation Accuracy: 29.7030\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.9739 Validation Accuracy: 33.2921\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.9346 Validation Accuracy: 31.5594\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.8846 Validation Accuracy: 34.4059\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.8479 Validation Accuracy: 35.5198\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.8213 Validation Accuracy: 36.2624\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.7793 Validation Accuracy: 37.1287\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.7580 Validation Accuracy: 36.8812\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.7336 Validation Accuracy: 38.3663\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.7139 Validation Accuracy: 39.8515\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.6885 Validation Accuracy: 40.0990\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.6752 Validation Accuracy: 40.2228\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.6562 Validation Accuracy: 41.5842\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.6398 Validation Accuracy: 43.0693\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.6260 Validation Accuracy: 43.8119\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.6111 Validation Accuracy: 45.0495\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.6075 Validation Accuracy: 45.0495\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.5795 Validation Accuracy: 46.0396\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.5837 Validation Accuracy: 46.9059\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.5614 Validation Accuracy: 47.6485\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.5542 Validation Accuracy: 48.1436\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.5359 Validation Accuracy: 48.6386\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.5338 Validation Accuracy: 47.4010\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.5191 Validation Accuracy: 48.7624\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.5162 Validation Accuracy: 50.1238\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.4984 Validation Accuracy: 49.7525\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.4843 Validation Accuracy: 49.5050\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.4862 Validation Accuracy: 50.3713\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.4763 Validation Accuracy: 50.1238\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.4677 Validation Accuracy: 49.5050\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.4520 Validation Accuracy: 51.1139\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.4594 Validation Accuracy: 50.9901\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.4356 Validation Accuracy: 51.8564\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.4297 Validation Accuracy: 52.4752\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.4291 Validation Accuracy: 51.7327\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.4150 Validation Accuracy: 51.6089\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.4214 Validation Accuracy: 52.1040\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.3875 Validation Accuracy: 52.7228\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.3971 Validation Accuracy: 51.7327\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.3933 Validation Accuracy: 53.0941\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.3737 Validation Accuracy: 53.7129\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.3749 Validation Accuracy: 54.0842\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.3682 Validation Accuracy: 54.3317\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.3573 Validation Accuracy: 54.5792\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.3632 Validation Accuracy: 54.0842\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.3437 Validation Accuracy: 55.3218\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.3522 Validation Accuracy: 55.9406\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.3300 Validation Accuracy: 55.3218\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.3306 Validation Accuracy: 56.3119\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.3267 Validation Accuracy: 55.5693\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.3182 Validation Accuracy: 56.0644\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.3012 Validation Accuracy: 56.6832\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.2899 Validation Accuracy: 57.4257\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.2937 Validation Accuracy: 56.9307\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.2849 Validation Accuracy: 57.0545\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.2853 Validation Accuracy: 57.1782\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.2697 Validation Accuracy: 58.7871\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.2715 Validation Accuracy: 58.6634\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.2435 Validation Accuracy: 59.6535\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.2472 Validation Accuracy: 61.3861\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.2396 Validation Accuracy: 61.2624\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.2238 Validation Accuracy: 60.2723\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.2288 Validation Accuracy: 60.6436\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.2225 Validation Accuracy: 59.0347\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.2101 Validation Accuracy: 61.7574\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.1966 Validation Accuracy: 62.8713\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.1759 Validation Accuracy: 64.6040\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.1712 Validation Accuracy: 65.2228\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.1640 Validation Accuracy: 64.8515\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.1661 Validation Accuracy: 64.1089\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.1403 Validation Accuracy: 65.4703\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.1449 Validation Accuracy: 65.3465\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.1357 Validation Accuracy: 65.5941\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.1261 Validation Accuracy: 64.9752\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.1218 Validation Accuracy: 63.9852\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.1237 Validation Accuracy: 63.6139\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.1078 Validation Accuracy: 66.5842\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.1140 Validation Accuracy: 65.9653\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.1186 Validation Accuracy: 65.2228\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.1092 Validation Accuracy: 64.7277\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.0795 Validation Accuracy: 66.2129\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.0809 Validation Accuracy: 66.3366\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.0542 Validation Accuracy: 66.5842\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.0676 Validation Accuracy: 68.1931\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.0665 Validation Accuracy: 66.2129\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.0520 Validation Accuracy: 68.5644\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.0653 Validation Accuracy: 67.2030\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.0548 Validation Accuracy: 68.0693\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.0402 Validation Accuracy: 69.1832\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.0259 Validation Accuracy: 69.9257\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2942 Validation Accuracy: 13.1188\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.2853 Validation Accuracy: 12.2525\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 2.2527 Validation Accuracy: 18.3168\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 2.1903 Validation Accuracy: 19.9257\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 2.1196 Validation Accuracy: 23.6386\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.0642 Validation Accuracy: 23.0198\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 2.0068 Validation Accuracy: 26.2376\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.9705 Validation Accuracy: 28.2178\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.9354 Validation Accuracy: 27.9703\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.9366 Validation Accuracy: 30.1980\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.9301 Validation Accuracy: 26.6089\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.8987 Validation Accuracy: 30.0743\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.8735 Validation Accuracy: 29.9505\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.8541 Validation Accuracy: 31.6832\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.8651 Validation Accuracy: 31.9307\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.8544 Validation Accuracy: 30.9406\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.8284 Validation Accuracy: 33.2921\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.7969 Validation Accuracy: 35.5198\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.7888 Validation Accuracy: 35.1485\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.8057 Validation Accuracy: 31.4356\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.7877 Validation Accuracy: 34.7772\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.7866 Validation Accuracy: 34.0347\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.7454 Validation Accuracy: 37.6238\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.7371 Validation Accuracy: 38.3663\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.7625 Validation Accuracy: 33.1683\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.7446 Validation Accuracy: 36.2624\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.7500 Validation Accuracy: 36.3861\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 1.6918 Validation Accuracy: 40.4703\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 1.7046 Validation Accuracy: 37.7475\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 1.7325 Validation Accuracy: 34.9010\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.7121 Validation Accuracy: 37.9950\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 1.7142 Validation Accuracy: 37.3762\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 1.6462 Validation Accuracy: 42.6980\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 1.6789 Validation Accuracy: 38.2426\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 1.7082 Validation Accuracy: 36.8812\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.6838 Validation Accuracy: 40.9653\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 1.6777 Validation Accuracy: 38.3663\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 1.6062 Validation Accuracy: 45.5446\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 1.6343 Validation Accuracy: 40.5941\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 1.6737 Validation Accuracy: 39.1089\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.6498 Validation Accuracy: 41.8317\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 1.6505 Validation Accuracy: 39.2327\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 1.5763 Validation Accuracy: 47.6485\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 1.6068 Validation Accuracy: 41.3366\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 1.6468 Validation Accuracy: 40.7178\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.6169 Validation Accuracy: 42.9455\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 1.6221 Validation Accuracy: 41.5842\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 1.5479 Validation Accuracy: 48.6386\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 1.5789 Validation Accuracy: 43.0693\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 1.6208 Validation Accuracy: 41.7079\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.5843 Validation Accuracy: 43.5644\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 1.5995 Validation Accuracy: 42.6980\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 1.5215 Validation Accuracy: 48.5149\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 1.5598 Validation Accuracy: 43.0693\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 1.6047 Validation Accuracy: 42.0792\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.5651 Validation Accuracy: 43.5644\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 1.5772 Validation Accuracy: 44.0594\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 1.5028 Validation Accuracy: 48.8861\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 1.5360 Validation Accuracy: 43.5644\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 1.5818 Validation Accuracy: 43.6881\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.5439 Validation Accuracy: 44.8020\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 1.5619 Validation Accuracy: 43.4406\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 1.4814 Validation Accuracy: 49.0099\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 1.5228 Validation Accuracy: 45.2970\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 1.5674 Validation Accuracy: 44.0594\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.5274 Validation Accuracy: 44.4307\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 1.5466 Validation Accuracy: 44.0594\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 1.4699 Validation Accuracy: 49.2574\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 1.5017 Validation Accuracy: 45.9158\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 1.5465 Validation Accuracy: 45.1733\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.5114 Validation Accuracy: 45.4208\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 1.5343 Validation Accuracy: 44.3069\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 1.4509 Validation Accuracy: 49.2574\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 1.4893 Validation Accuracy: 46.1634\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 1.5326 Validation Accuracy: 45.4208\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.5012 Validation Accuracy: 45.6683\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 1.5243 Validation Accuracy: 45.6683\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 1.4332 Validation Accuracy: 49.0099\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 1.4778 Validation Accuracy: 46.7822\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 1.5227 Validation Accuracy: 46.9059\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.4776 Validation Accuracy: 46.6584\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 1.5036 Validation Accuracy: 46.4109\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 1.4229 Validation Accuracy: 50.1238\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 1.4634 Validation Accuracy: 47.4010\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 1.5067 Validation Accuracy: 47.1535\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.4708 Validation Accuracy: 48.0198\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 1.4924 Validation Accuracy: 46.0396\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 1.4091 Validation Accuracy: 49.3812\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 1.4557 Validation Accuracy: 47.6485\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 1.4896 Validation Accuracy: 47.8960\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.4584 Validation Accuracy: 48.1436\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 1.4836 Validation Accuracy: 47.1535\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 1.3922 Validation Accuracy: 50.0000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 1.4454 Validation Accuracy: 48.0198\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 1.4856 Validation Accuracy: 47.8960\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 1.4456 Validation Accuracy: 49.0099\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 1.4642 Validation Accuracy: 47.7723\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 1.3854 Validation Accuracy: 51.2376\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 1.4231 Validation Accuracy: 49.8762\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 1.4645 Validation Accuracy: 49.2574\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.4348 Validation Accuracy: 49.7525\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 1.4594 Validation Accuracy: 47.8960\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 1.3736 Validation Accuracy: 50.9901\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 1.4115 Validation Accuracy: 50.2475\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 1.4564 Validation Accuracy: 49.0099\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.4239 Validation Accuracy: 49.5050\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 1.4460 Validation Accuracy: 48.8861\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 1.3615 Validation Accuracy: 52.7228\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 1.4057 Validation Accuracy: 49.7525\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 1.4419 Validation Accuracy: 49.5050\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 1.4091 Validation Accuracy: 49.3812\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 1.4345 Validation Accuracy: 49.3812\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 1.3523 Validation Accuracy: 52.8465\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 1.3885 Validation Accuracy: 51.1139\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 1.4357 Validation Accuracy: 49.5050\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 1.3979 Validation Accuracy: 50.1238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 1.4223 Validation Accuracy: 50.4951\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 1.3392 Validation Accuracy: 53.4653\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 1.3737 Validation Accuracy: 52.3515\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 1.4301 Validation Accuracy: 49.3812\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 1.4010 Validation Accuracy: 51.3614\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 1.4141 Validation Accuracy: 49.8762\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 1.3224 Validation Accuracy: 53.2178\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 1.3658 Validation Accuracy: 50.9901\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 1.4223 Validation Accuracy: 49.6287\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 1.3803 Validation Accuracy: 51.3614\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 1.4041 Validation Accuracy: 50.9901\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 1.3206 Validation Accuracy: 53.8366\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 1.3598 Validation Accuracy: 52.7228\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 1.4100 Validation Accuracy: 50.1238\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 1.3733 Validation Accuracy: 51.3614\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 1.3964 Validation Accuracy: 50.7426\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 1.3155 Validation Accuracy: 54.3317\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 1.3442 Validation Accuracy: 51.9802\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 1.4023 Validation Accuracy: 50.6188\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 1.3611 Validation Accuracy: 52.9703\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 1.3809 Validation Accuracy: 50.7426\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 1.3000 Validation Accuracy: 55.3218\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 1.3366 Validation Accuracy: 53.3416\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 1.3909 Validation Accuracy: 50.2475\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 1.3449 Validation Accuracy: 53.4653\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 1.3775 Validation Accuracy: 52.7228\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 1.3008 Validation Accuracy: 53.5891\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 1.3326 Validation Accuracy: 52.9703\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 1.3846 Validation Accuracy: 50.9901\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 1.3462 Validation Accuracy: 53.0941\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 1.3699 Validation Accuracy: 51.9802\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 1.2958 Validation Accuracy: 55.6931\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 1.3233 Validation Accuracy: 53.2178\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 1.3701 Validation Accuracy: 51.1139\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 1.3289 Validation Accuracy: 54.2079\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 1.3600 Validation Accuracy: 52.7228\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 1.2814 Validation Accuracy: 55.4455\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 1.3061 Validation Accuracy: 53.0941\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 1.3678 Validation Accuracy: 51.3614\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 1.3311 Validation Accuracy: 54.3317\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 1.3434 Validation Accuracy: 53.5891\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 1.2696 Validation Accuracy: 56.0644\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 1.3087 Validation Accuracy: 55.5693\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 1.3506 Validation Accuracy: 52.2277\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 1.3198 Validation Accuracy: 55.0743\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 1.3488 Validation Accuracy: 52.7228\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 1.2689 Validation Accuracy: 56.9307\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 1.2971 Validation Accuracy: 54.7030\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 1.3448 Validation Accuracy: 52.2277\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 1.3069 Validation Accuracy: 54.7030\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 1.3294 Validation Accuracy: 53.7129\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 1.2640 Validation Accuracy: 56.6832\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 1.2818 Validation Accuracy: 55.5693\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 1.3353 Validation Accuracy: 52.4752\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 1.3101 Validation Accuracy: 54.9505\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 1.3305 Validation Accuracy: 54.5792\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 1.2379 Validation Accuracy: 56.9307\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 1.2825 Validation Accuracy: 55.0743\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 1.3367 Validation Accuracy: 52.5990\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 1.3010 Validation Accuracy: 56.0644\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 1.3141 Validation Accuracy: 55.5693\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 1.2384 Validation Accuracy: 56.4356\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 1.2850 Validation Accuracy: 55.3218\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 1.3222 Validation Accuracy: 53.3416\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 1.2902 Validation Accuracy: 56.0644\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 1.3105 Validation Accuracy: 55.6931\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 1.2388 Validation Accuracy: 56.8069\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 1.2714 Validation Accuracy: 55.4455\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 1.3158 Validation Accuracy: 53.7129\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 1.2867 Validation Accuracy: 57.0545\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 1.3043 Validation Accuracy: 54.7030\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 1.2426 Validation Accuracy: 56.1881\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 1.2744 Validation Accuracy: 55.8168\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 1.2972 Validation Accuracy: 53.5891\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 1.2761 Validation Accuracy: 56.4356\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 1.2994 Validation Accuracy: 55.8168\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 1.2268 Validation Accuracy: 56.6832\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 1.2641 Validation Accuracy: 55.5693\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 1.3078 Validation Accuracy: 53.5891\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 1.2781 Validation Accuracy: 57.3020\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 1.2869 Validation Accuracy: 56.6832\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 1.2240 Validation Accuracy: 57.4257\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 1.2591 Validation Accuracy: 57.1782\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 1.2878 Validation Accuracy: 54.2079\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 1.2745 Validation Accuracy: 57.1782\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 1.2811 Validation Accuracy: 55.6931\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 1.2182 Validation Accuracy: 58.0446\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 1.2490 Validation Accuracy: 57.6733\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 1.2844 Validation Accuracy: 55.1980\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 1.2678 Validation Accuracy: 57.3020\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 1.2751 Validation Accuracy: 56.8069\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 1.2109 Validation Accuracy: 57.7970\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 1.2522 Validation Accuracy: 58.1683\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 1.2754 Validation Accuracy: 54.4554\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 1.2546 Validation Accuracy: 57.7970\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 1.2657 Validation Accuracy: 57.3020\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 1.2010 Validation Accuracy: 57.7970\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 1.2456 Validation Accuracy: 57.5495\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 1.2709 Validation Accuracy: 54.7030\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 1.2555 Validation Accuracy: 57.0545\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 1.2669 Validation Accuracy: 56.9307\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 1.1966 Validation Accuracy: 58.1683\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 1.2287 Validation Accuracy: 58.4158\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 1.2721 Validation Accuracy: 54.3317\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 1.2417 Validation Accuracy: 56.5594\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 1.2611 Validation Accuracy: 57.7970\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 1.2173 Validation Accuracy: 57.4257\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 1.2241 Validation Accuracy: 58.6634\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 1.2497 Validation Accuracy: 55.4455\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 1.2497 Validation Accuracy: 57.4257\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 1.2462 Validation Accuracy: 57.5495\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 1.1849 Validation Accuracy: 58.9109\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 1.2198 Validation Accuracy: 60.2723\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 1.2588 Validation Accuracy: 55.4455\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 1.2344 Validation Accuracy: 58.4158\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 1.2446 Validation Accuracy: 56.9307\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 1.1907 Validation Accuracy: 58.6634\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 1.2148 Validation Accuracy: 58.1683\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 1.2409 Validation Accuracy: 55.0743\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 1.2421 Validation Accuracy: 57.7970\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 1.2476 Validation Accuracy: 58.0446\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 1.1749 Validation Accuracy: 58.1683\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 1.2125 Validation Accuracy: 59.9010\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 1.2509 Validation Accuracy: 54.0842\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 1.2364 Validation Accuracy: 58.0446\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 1.2351 Validation Accuracy: 57.4257\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 1.1759 Validation Accuracy: 58.5396\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 1.2117 Validation Accuracy: 59.4059\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 1.2418 Validation Accuracy: 55.9406\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 1.2229 Validation Accuracy: 58.5396\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 1.2295 Validation Accuracy: 57.0545\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 1.1747 Validation Accuracy: 59.5297\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 1.2050 Validation Accuracy: 60.2723\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 1.2258 Validation Accuracy: 56.6832\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 1.2261 Validation Accuracy: 58.1683\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 1.2301 Validation Accuracy: 58.0446\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 1.1734 Validation Accuracy: 60.2723\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 1.1943 Validation Accuracy: 59.5297\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 1.2230 Validation Accuracy: 56.5594\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 1.2316 Validation Accuracy: 58.2921\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 1.2145 Validation Accuracy: 57.9208\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 1.1607 Validation Accuracy: 59.9010\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 1.2025 Validation Accuracy: 61.2624\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 1.2127 Validation Accuracy: 57.0545\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 1.2086 Validation Accuracy: 58.6634\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 1.2169 Validation Accuracy: 57.6733\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 1.1627 Validation Accuracy: 60.6436\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 1.1904 Validation Accuracy: 61.0148\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 1.2081 Validation Accuracy: 56.5594\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 1.2067 Validation Accuracy: 58.9109\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 1.2133 Validation Accuracy: 58.4158\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 1.1602 Validation Accuracy: 60.5198\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 1.1775 Validation Accuracy: 60.1485\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 1.2010 Validation Accuracy: 56.6832\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 1.2078 Validation Accuracy: 59.1584\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 1.2102 Validation Accuracy: 58.4158\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 1.1488 Validation Accuracy: 59.6535\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 1.1834 Validation Accuracy: 61.1386\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 1.2034 Validation Accuracy: 57.3020\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 1.2026 Validation Accuracy: 59.2822\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 1.1999 Validation Accuracy: 58.2921\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 1.1575 Validation Accuracy: 59.5297\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 1.1743 Validation Accuracy: 61.7574\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 1.1955 Validation Accuracy: 56.5594\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 1.2007 Validation Accuracy: 58.9109\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 1.1988 Validation Accuracy: 59.1584\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 1.1436 Validation Accuracy: 60.2723\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 1.1697 Validation Accuracy: 62.2525\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 1.1860 Validation Accuracy: 57.5495\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 1.1920 Validation Accuracy: 58.7871\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 1.1934 Validation Accuracy: 59.9010\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 1.1423 Validation Accuracy: 61.0148\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 1.1647 Validation Accuracy: 61.2624\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 1.1854 Validation Accuracy: 57.4257\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 1.1901 Validation Accuracy: 58.7871\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 1.1892 Validation Accuracy: 58.4158\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 1.1367 Validation Accuracy: 61.1386\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 1.1591 Validation Accuracy: 61.8812\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 1.1852 Validation Accuracy: 57.7970\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 1.1854 Validation Accuracy: 58.7871\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 1.1822 Validation Accuracy: 59.6535\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 1.1427 Validation Accuracy: 60.5198\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 1.1652 Validation Accuracy: 61.7574\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 1.1781 Validation Accuracy: 57.0545\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 1.1876 Validation Accuracy: 58.7871\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 1.1808 Validation Accuracy: 59.5297\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 1.1258 Validation Accuracy: 61.7574\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 1.1610 Validation Accuracy: 62.6238\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 1.1810 Validation Accuracy: 58.2921\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 1.1856 Validation Accuracy: 59.0347\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 1.1794 Validation Accuracy: 59.1584\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 1.1277 Validation Accuracy: 61.0148\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 1.1473 Validation Accuracy: 62.6238\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 1.1711 Validation Accuracy: 58.1683\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 1.1761 Validation Accuracy: 59.5297\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 1.1717 Validation Accuracy: 59.6535\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 1.1319 Validation Accuracy: 61.5099\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 1.1379 Validation Accuracy: 62.0049\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 1.1623 Validation Accuracy: 58.2921\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 1.1733 Validation Accuracy: 59.7772\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 1.1757 Validation Accuracy: 58.6634\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 1.1317 Validation Accuracy: 61.1386\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 1.1413 Validation Accuracy: 62.6238\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 1.1575 Validation Accuracy: 58.2921\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 1.1749 Validation Accuracy: 59.2822\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 1.1639 Validation Accuracy: 60.2723\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 1.1169 Validation Accuracy: 61.6337\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 1.1441 Validation Accuracy: 62.0049\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 1.1520 Validation Accuracy: 58.9109\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 1.1583 Validation Accuracy: 59.5297\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 1.1731 Validation Accuracy: 60.8911\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 1.1197 Validation Accuracy: 62.2525\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 1.1400 Validation Accuracy: 61.8812\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 1.1531 Validation Accuracy: 58.9109\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 1.1596 Validation Accuracy: 59.2822\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 1.1715 Validation Accuracy: 60.3960\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 1.1201 Validation Accuracy: 61.7574\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 1.1293 Validation Accuracy: 63.2426\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 1.1398 Validation Accuracy: 59.1584\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 1.1554 Validation Accuracy: 58.6634\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 1.1623 Validation Accuracy: 61.8812\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 1.1072 Validation Accuracy: 61.5099\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 1.1318 Validation Accuracy: 61.6337\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 1.1399 Validation Accuracy: 60.0248\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 1.1514 Validation Accuracy: 60.6436\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 1.1491 Validation Accuracy: 59.6535\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 1.1067 Validation Accuracy: 63.2426\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 1.1221 Validation Accuracy: 62.2525\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 1.1391 Validation Accuracy: 60.0248\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 1.1521 Validation Accuracy: 60.3960\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 1.1537 Validation Accuracy: 60.5198\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 1.0990 Validation Accuracy: 62.8713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 1.1230 Validation Accuracy: 63.6139\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 1.1426 Validation Accuracy: 59.1584\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 1.1506 Validation Accuracy: 59.9010\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 1.1515 Validation Accuracy: 61.3861\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 1.1018 Validation Accuracy: 61.7574\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 1.1224 Validation Accuracy: 62.1287\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 1.1284 Validation Accuracy: 59.5297\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 1.1349 Validation Accuracy: 61.3861\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 1.1348 Validation Accuracy: 61.0148\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 1.1006 Validation Accuracy: 62.9951\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 1.1244 Validation Accuracy: 63.6139\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 1.1233 Validation Accuracy: 59.9010\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 1.1358 Validation Accuracy: 60.7673\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 1.1374 Validation Accuracy: 62.0049\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 1.0865 Validation Accuracy: 62.9951\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 1.1142 Validation Accuracy: 63.1188\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 1.1275 Validation Accuracy: 60.0248\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 1.1370 Validation Accuracy: 59.9010\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 1.1383 Validation Accuracy: 62.2525\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 1.0935 Validation Accuracy: 62.9951\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 1.1108 Validation Accuracy: 63.2426\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 1.1230 Validation Accuracy: 60.2723\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 1.1300 Validation Accuracy: 60.7673\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 1.1267 Validation Accuracy: 61.7574\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 1.1047 Validation Accuracy: 62.7475\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 1.1099 Validation Accuracy: 64.7277\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 1.1202 Validation Accuracy: 60.3960\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 1.1327 Validation Accuracy: 60.6436\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 1.1290 Validation Accuracy: 62.0049\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 1.0893 Validation Accuracy: 62.3762\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 1.1101 Validation Accuracy: 63.3663\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 1.1114 Validation Accuracy: 60.6436\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 1.1336 Validation Accuracy: 61.0148\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 1.1286 Validation Accuracy: 61.8812\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 1.0873 Validation Accuracy: 62.9951\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 1.1081 Validation Accuracy: 63.6139\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 1.1162 Validation Accuracy: 61.2624\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 1.1236 Validation Accuracy: 60.5198\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 1.1271 Validation Accuracy: 61.7574\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 1.0835 Validation Accuracy: 62.2525\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 1.0932 Validation Accuracy: 64.7277\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 1.1140 Validation Accuracy: 60.2723\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 1.1213 Validation Accuracy: 61.1386\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 1.1158 Validation Accuracy: 61.7574\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 1.0891 Validation Accuracy: 64.1089\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 1.0941 Validation Accuracy: 64.8515\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 1.1072 Validation Accuracy: 61.1386\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 1.1317 Validation Accuracy: 59.7772\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 1.1179 Validation Accuracy: 60.8911\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 1.0861 Validation Accuracy: 62.6238\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 1.0952 Validation Accuracy: 64.3564\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 1.1095 Validation Accuracy: 60.8911\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 1.1303 Validation Accuracy: 61.0148\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 1.1144 Validation Accuracy: 62.1287\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 1.0752 Validation Accuracy: 62.1287\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 1.1075 Validation Accuracy: 63.6139\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 1.1019 Validation Accuracy: 61.3861\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 1.1142 Validation Accuracy: 61.7574\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 1.1095 Validation Accuracy: 62.1287\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 1.0685 Validation Accuracy: 64.8515\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 1.0886 Validation Accuracy: 64.1089\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 1.0968 Validation Accuracy: 60.5198\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 1.1144 Validation Accuracy: 60.7673\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 1.1108 Validation Accuracy: 62.6238\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 1.0663 Validation Accuracy: 64.3564\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 1.0908 Validation Accuracy: 63.9852\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 1.1062 Validation Accuracy: 61.7574\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 1.1180 Validation Accuracy: 60.2723\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 1.1034 Validation Accuracy: 62.2525\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 1.0698 Validation Accuracy: 63.9852\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 1.0782 Validation Accuracy: 64.9752\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 1.0937 Validation Accuracy: 62.3762\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 1.1140 Validation Accuracy: 60.3960\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 1.0969 Validation Accuracy: 62.3762\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 1.0516 Validation Accuracy: 63.6139\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 1.1001 Validation Accuracy: 64.2327\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 1.1000 Validation Accuracy: 62.0049\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 1.1041 Validation Accuracy: 61.5099\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 1.1077 Validation Accuracy: 62.7475\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 1.0631 Validation Accuracy: 63.3663\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 1.0911 Validation Accuracy: 63.6139\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 1.0949 Validation Accuracy: 60.8911\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 1.0962 Validation Accuracy: 61.7574\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 1.0977 Validation Accuracy: 62.6238\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 1.0637 Validation Accuracy: 62.9951\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 1.0730 Validation Accuracy: 64.2327\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 1.0825 Validation Accuracy: 63.7376\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 1.1010 Validation Accuracy: 62.7475\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 1.0951 Validation Accuracy: 62.9951\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 1.0583 Validation Accuracy: 64.8515\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 1.0717 Validation Accuracy: 63.7376\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 1.0786 Validation Accuracy: 62.6238\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 1.0994 Validation Accuracy: 62.1287\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 1.0896 Validation Accuracy: 63.7376\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 1.0560 Validation Accuracy: 64.2327\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 1.0662 Validation Accuracy: 64.2327\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 1.0712 Validation Accuracy: 63.2426\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 1.0936 Validation Accuracy: 61.3861\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 1.0898 Validation Accuracy: 63.4901\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 1.0540 Validation Accuracy: 64.2327\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 1.0736 Validation Accuracy: 63.9852\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 1.0734 Validation Accuracy: 63.1188\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 1.0828 Validation Accuracy: 63.1188\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 1.0930 Validation Accuracy: 63.3663\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 1.0466 Validation Accuracy: 65.3465\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 1.0699 Validation Accuracy: 62.8713\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 1.0784 Validation Accuracy: 62.7475\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 1.0821 Validation Accuracy: 62.1287\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 1.0851 Validation Accuracy: 63.6139\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 1.0341 Validation Accuracy: 65.2228\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 1.0668 Validation Accuracy: 64.2327\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 1.0832 Validation Accuracy: 62.8713\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 1.0810 Validation Accuracy: 63.1188\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 1.0865 Validation Accuracy: 63.6139\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 1.0464 Validation Accuracy: 65.3465\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 1.0694 Validation Accuracy: 63.9852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 1.0652 Validation Accuracy: 62.9951\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 1.0791 Validation Accuracy: 62.9951\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 1.0835 Validation Accuracy: 63.8614\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 1.0459 Validation Accuracy: 64.9752\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 1.0507 Validation Accuracy: 64.8515\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 1.0642 Validation Accuracy: 63.8614\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 1.0748 Validation Accuracy: 63.2426\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 1.0753 Validation Accuracy: 64.1089\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 1.0299 Validation Accuracy: 65.7178\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 1.0553 Validation Accuracy: 64.4802\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 1.0823 Validation Accuracy: 62.3762\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 1.0722 Validation Accuracy: 63.1188\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 1.0799 Validation Accuracy: 63.4901\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 1.0370 Validation Accuracy: 64.9752\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 1.0564 Validation Accuracy: 62.9951\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 1.0590 Validation Accuracy: 63.8614\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 1.0803 Validation Accuracy: 62.7475\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 1.0705 Validation Accuracy: 63.9852\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 1.0350 Validation Accuracy: 65.8416\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 1.0535 Validation Accuracy: 65.3465\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 1.0505 Validation Accuracy: 63.7376\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 1.0708 Validation Accuracy: 63.2426\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 1.0747 Validation Accuracy: 62.7475\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 1.0332 Validation Accuracy: 64.9752\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 1.0526 Validation Accuracy: 64.4802\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 1.0518 Validation Accuracy: 63.4901\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 1.0602 Validation Accuracy: 63.6139\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 1.0718 Validation Accuracy: 63.6139\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 1.0254 Validation Accuracy: 64.8515\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 1.0497 Validation Accuracy: 64.7277\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 1.0541 Validation Accuracy: 63.9852\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 1.0579 Validation Accuracy: 63.6139\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 1.0656 Validation Accuracy: 63.9852\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 1.0297 Validation Accuracy: 66.0891\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 1.0513 Validation Accuracy: 64.3564\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 1.0554 Validation Accuracy: 63.7376\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 1.0672 Validation Accuracy: 64.3564\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 1.0685 Validation Accuracy: 63.4901\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 1.0297 Validation Accuracy: 66.0891\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 1.0483 Validation Accuracy: 63.8614\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 1.0478 Validation Accuracy: 62.9951\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 1.0689 Validation Accuracy: 63.4901\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 1.0680 Validation Accuracy: 63.6139\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 1.0224 Validation Accuracy: 66.3366\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 1.0359 Validation Accuracy: 65.9653\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 1.0449 Validation Accuracy: 64.6040\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 1.0559 Validation Accuracy: 63.4901\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 1.0593 Validation Accuracy: 64.6040\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 1.0162 Validation Accuracy: 67.2030\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 1.0421 Validation Accuracy: 64.4802\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 1.0449 Validation Accuracy: 65.2228\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 1.0426 Validation Accuracy: 63.3663\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 1.0597 Validation Accuracy: 63.8614\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 1.0323 Validation Accuracy: 65.3465\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 1.0523 Validation Accuracy: 64.8515\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 1.0580 Validation Accuracy: 64.3564\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 1.0470 Validation Accuracy: 65.4703\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 1.0594 Validation Accuracy: 64.4802\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 1.0204 Validation Accuracy: 66.5842\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 1.0369 Validation Accuracy: 65.0990\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 1.0354 Validation Accuracy: 64.3564\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 1.0509 Validation Accuracy: 62.9951\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 1.0531 Validation Accuracy: 64.3564\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 1.0125 Validation Accuracy: 67.0792\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 1.0417 Validation Accuracy: 65.4703\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 1.0370 Validation Accuracy: 64.3564\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 1.0524 Validation Accuracy: 63.9852\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 1.0501 Validation Accuracy: 64.6040\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 1.0170 Validation Accuracy: 65.8416\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 1.0420 Validation Accuracy: 65.5941\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 1.0369 Validation Accuracy: 64.7277\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 1.0375 Validation Accuracy: 63.7376\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 1.0485 Validation Accuracy: 64.2327\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 1.0122 Validation Accuracy: 66.2129\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 1.0432 Validation Accuracy: 65.3465\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 1.0350 Validation Accuracy: 63.9852\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 1.0430 Validation Accuracy: 65.2228\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 1.0458 Validation Accuracy: 64.7277\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 1.0008 Validation Accuracy: 66.9554\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 1.0288 Validation Accuracy: 65.7178\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 1.0345 Validation Accuracy: 64.9752\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 1.0363 Validation Accuracy: 64.3564\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 1.0449 Validation Accuracy: 64.2327\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 1.0101 Validation Accuracy: 68.3168\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 1.0292 Validation Accuracy: 65.8416\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 1.0247 Validation Accuracy: 64.7277\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 1.0396 Validation Accuracy: 64.2327\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 1.0358 Validation Accuracy: 64.3564\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 1.0034 Validation Accuracy: 65.9653\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 1.0277 Validation Accuracy: 65.7178\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 1.0177 Validation Accuracy: 65.4703\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 1.0343 Validation Accuracy: 64.9752\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 1.0349 Validation Accuracy: 64.7277\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 0.9996 Validation Accuracy: 67.8218\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 1.0156 Validation Accuracy: 67.0792\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 1.0219 Validation Accuracy: 65.9653\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 1.0309 Validation Accuracy: 65.3465\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 1.0343 Validation Accuracy: 64.8515\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 1.0116 Validation Accuracy: 66.3366\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 1.0362 Validation Accuracy: 65.7178\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 1.0240 Validation Accuracy: 65.7178\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 1.0387 Validation Accuracy: 64.6040\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 1.0437 Validation Accuracy: 64.9752\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 1.0055 Validation Accuracy: 67.3267\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 1.0166 Validation Accuracy: 65.7178\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 1.0264 Validation Accuracy: 65.7178\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 1.0234 Validation Accuracy: 65.5941\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 1.0284 Validation Accuracy: 65.0990\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 0.9962 Validation Accuracy: 68.8119\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 1.0285 Validation Accuracy: 66.0891\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 1.0091 Validation Accuracy: 65.2228\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 1.0189 Validation Accuracy: 65.8416\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 1.0375 Validation Accuracy: 64.4802\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 0.9906 Validation Accuracy: 67.5743\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 1.0079 Validation Accuracy: 66.2129\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 1.0187 Validation Accuracy: 64.4802\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 1.0342 Validation Accuracy: 65.3465\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 1.0329 Validation Accuracy: 65.0990\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 0.9843 Validation Accuracy: 67.3267\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 1.0177 Validation Accuracy: 66.5842\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 1.0155 Validation Accuracy: 66.9554\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 1.0186 Validation Accuracy: 64.8515\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 1.0272 Validation Accuracy: 64.8515\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 0.9960 Validation Accuracy: 66.8317\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 1.0169 Validation Accuracy: 66.4604\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 1.0148 Validation Accuracy: 65.3465\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 1.0378 Validation Accuracy: 64.1089\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 1.0349 Validation Accuracy: 65.5941\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 0.9956 Validation Accuracy: 65.9653\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 1.0083 Validation Accuracy: 66.8317\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 1.0075 Validation Accuracy: 66.7079\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 1.0169 Validation Accuracy: 64.8515\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 1.0248 Validation Accuracy: 64.7277\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 0.9938 Validation Accuracy: 68.0693\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 1.0104 Validation Accuracy: 65.7178\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 1.0116 Validation Accuracy: 66.7079\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 1.0224 Validation Accuracy: 65.5941\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 1.0205 Validation Accuracy: 65.8416\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 0.9824 Validation Accuracy: 68.6881\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 1.0079 Validation Accuracy: 66.0891\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 0.9992 Validation Accuracy: 66.7079\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 1.0187 Validation Accuracy: 65.3465\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 1.0167 Validation Accuracy: 65.3465\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 0.9933 Validation Accuracy: 66.8317\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 1.0095 Validation Accuracy: 67.2030\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 1.0000 Validation Accuracy: 65.9653\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 1.0033 Validation Accuracy: 65.2228\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 1.0263 Validation Accuracy: 65.7178\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 0.9673 Validation Accuracy: 68.6881\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.9987 Validation Accuracy: 66.9554\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 1.0122 Validation Accuracy: 65.9653\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 1.0156 Validation Accuracy: 64.8515\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 1.0255 Validation Accuracy: 66.2129\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 0.9917 Validation Accuracy: 67.3267\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 0.9935 Validation Accuracy: 67.2030\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 0.9969 Validation Accuracy: 66.9554\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 1.0094 Validation Accuracy: 66.3366\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 1.0237 Validation Accuracy: 65.4703\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 0.9757 Validation Accuracy: 69.0594\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.9917 Validation Accuracy: 66.5842\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 1.0026 Validation Accuracy: 66.3366\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 1.0067 Validation Accuracy: 66.0891\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 1.0057 Validation Accuracy: 65.3465\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 0.9918 Validation Accuracy: 68.0693\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.9918 Validation Accuracy: 66.4604\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 0.9909 Validation Accuracy: 66.0891\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 1.0169 Validation Accuracy: 66.4604\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 1.0101 Validation Accuracy: 66.3366\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 0.9683 Validation Accuracy: 68.8119\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.9965 Validation Accuracy: 66.7079\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 0.9889 Validation Accuracy: 66.3366\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 1.0054 Validation Accuracy: 65.8416\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 1.0104 Validation Accuracy: 65.9653\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 0.9699 Validation Accuracy: 68.1931\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.9951 Validation Accuracy: 66.9554\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 0.9994 Validation Accuracy: 67.0792\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 1.0065 Validation Accuracy: 66.5842\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 1.0030 Validation Accuracy: 65.4703\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.9652 Validation Accuracy: 68.8119\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 1.0004 Validation Accuracy: 66.3366\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 0.9908 Validation Accuracy: 67.0792\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 1.0029 Validation Accuracy: 64.9752\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 1.0164 Validation Accuracy: 65.8416\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.9700 Validation Accuracy: 67.4505\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.9910 Validation Accuracy: 66.4604\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 0.9893 Validation Accuracy: 67.6980\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.9927 Validation Accuracy: 66.8317\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 1.0014 Validation Accuracy: 65.9653\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 0.9713 Validation Accuracy: 69.1832\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.9823 Validation Accuracy: 68.3168\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 0.9882 Validation Accuracy: 67.2030\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 1.0045 Validation Accuracy: 67.0792\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 0.9984 Validation Accuracy: 66.0891\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.9725 Validation Accuracy: 69.5545\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.9917 Validation Accuracy: 67.4505\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 0.9820 Validation Accuracy: 68.0693\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.9964 Validation Accuracy: 67.2030\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 1.0055 Validation Accuracy: 66.9554\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 0.9707 Validation Accuracy: 67.9455\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.9954 Validation Accuracy: 68.0693\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 0.9935 Validation Accuracy: 66.4604\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 1.0051 Validation Accuracy: 65.5941\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 0.9999 Validation Accuracy: 65.7178\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.9589 Validation Accuracy: 68.6881\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.9891 Validation Accuracy: 67.4505\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 0.9937 Validation Accuracy: 66.2129\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.9913 Validation Accuracy: 67.5743\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 1.0015 Validation Accuracy: 66.2129\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 0.9654 Validation Accuracy: 68.4406\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.9829 Validation Accuracy: 67.3267\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 0.9833 Validation Accuracy: 67.8218\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.9906 Validation Accuracy: 67.0792\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 0.9986 Validation Accuracy: 65.9653\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 0.9547 Validation Accuracy: 69.4307\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.9768 Validation Accuracy: 67.3267\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 0.9819 Validation Accuracy: 66.4604\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.9914 Validation Accuracy: 66.9554\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 0.9940 Validation Accuracy: 66.3366\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 0.9589 Validation Accuracy: 68.8119\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.9799 Validation Accuracy: 67.8218\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 0.9638 Validation Accuracy: 68.4406\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.9904 Validation Accuracy: 66.5842\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.9960 Validation Accuracy: 65.9653\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 0.9488 Validation Accuracy: 69.0594\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.9723 Validation Accuracy: 68.5644\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 0.9802 Validation Accuracy: 68.5644\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.9973 Validation Accuracy: 65.2228\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 0.9939 Validation Accuracy: 65.9653\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.9661 Validation Accuracy: 67.9455\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.9816 Validation Accuracy: 67.5743\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 0.9704 Validation Accuracy: 67.9455\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.9910 Validation Accuracy: 66.2129\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 0.9963 Validation Accuracy: 66.3366\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 0.9519 Validation Accuracy: 69.1832\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.9835 Validation Accuracy: 67.5743\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 0.9698 Validation Accuracy: 67.5743\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.9841 Validation Accuracy: 66.8317\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 0.9977 Validation Accuracy: 66.9554\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.9451 Validation Accuracy: 69.3069\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.9690 Validation Accuracy: 67.8218\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 0.9687 Validation Accuracy: 68.8119\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.9818 Validation Accuracy: 67.0792\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 0.9924 Validation Accuracy: 66.5842\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 0.9342 Validation Accuracy: 69.1832\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.9678 Validation Accuracy: 68.0693\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 0.9769 Validation Accuracy: 68.9356\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.9781 Validation Accuracy: 66.4604\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 0.9854 Validation Accuracy: 67.4505\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.9528 Validation Accuracy: 70.2970\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.9633 Validation Accuracy: 68.4406\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 0.9723 Validation Accuracy: 67.8218\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.9738 Validation Accuracy: 66.9554\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.9804 Validation Accuracy: 67.0792\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.9442 Validation Accuracy: 70.1733\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.9691 Validation Accuracy: 68.3168\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 0.9664 Validation Accuracy: 67.6980\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.9733 Validation Accuracy: 66.9554\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 0.9865 Validation Accuracy: 68.1931\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.9433 Validation Accuracy: 69.4307\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.9644 Validation Accuracy: 68.1931\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 0.9661 Validation Accuracy: 68.8119\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.9765 Validation Accuracy: 67.0792\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 0.9863 Validation Accuracy: 67.6980\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.9402 Validation Accuracy: 69.8020\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.9589 Validation Accuracy: 67.4505\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 0.9809 Validation Accuracy: 68.3168\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.9822 Validation Accuracy: 66.7079\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 0.9775 Validation Accuracy: 66.5842\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.9435 Validation Accuracy: 68.6881\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.9703 Validation Accuracy: 68.5644\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 0.9721 Validation Accuracy: 68.8119\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.9772 Validation Accuracy: 65.8416\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.9802 Validation Accuracy: 67.6980\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.9471 Validation Accuracy: 67.8218\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.9576 Validation Accuracy: 68.8119\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 0.9679 Validation Accuracy: 68.3168\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.9815 Validation Accuracy: 65.5941\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 0.9878 Validation Accuracy: 67.8218\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 0.9311 Validation Accuracy: 70.2970\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.9683 Validation Accuracy: 68.4406\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 0.9735 Validation Accuracy: 68.8119\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.9750 Validation Accuracy: 66.8317\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 0.9815 Validation Accuracy: 67.6980\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.9375 Validation Accuracy: 69.8020\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.9483 Validation Accuracy: 67.5743\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 0.9641 Validation Accuracy: 67.6980\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.9642 Validation Accuracy: 67.8218\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 0.9754 Validation Accuracy: 66.8317\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 0.9313 Validation Accuracy: 70.0495\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 0.9637 Validation Accuracy: 68.6881\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 0.9668 Validation Accuracy: 68.4406\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.9684 Validation Accuracy: 66.7079\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 0.9801 Validation Accuracy: 67.2030\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 0.9329 Validation Accuracy: 69.5545\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 0.9633 Validation Accuracy: 68.1931\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 0.9609 Validation Accuracy: 69.6782\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.9703 Validation Accuracy: 67.9455\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 0.9703 Validation Accuracy: 67.9455\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 0.9267 Validation Accuracy: 70.6683\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 0.9526 Validation Accuracy: 68.1931\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 0.9538 Validation Accuracy: 69.4307\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.9633 Validation Accuracy: 67.5743\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 0.9743 Validation Accuracy: 66.8317\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 0.9230 Validation Accuracy: 70.0495\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 0.9461 Validation Accuracy: 68.8119\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 0.9583 Validation Accuracy: 68.8119\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.9621 Validation Accuracy: 67.9455\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 0.9772 Validation Accuracy: 67.8218\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 0.9391 Validation Accuracy: 69.3069\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 0.9410 Validation Accuracy: 69.4307\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 0.9501 Validation Accuracy: 69.8020\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.9709 Validation Accuracy: 67.5743\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 0.9630 Validation Accuracy: 66.7079\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 0.9285 Validation Accuracy: 70.6683\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 0.9565 Validation Accuracy: 68.4406\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 0.9440 Validation Accuracy: 68.5644\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.9728 Validation Accuracy: 67.6980\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 0.9631 Validation Accuracy: 67.3267\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 0.9169 Validation Accuracy: 70.4208\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 0.9501 Validation Accuracy: 68.9356\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 0.9510 Validation Accuracy: 68.8119\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.9651 Validation Accuracy: 66.8317\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 0.9735 Validation Accuracy: 67.8218\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 0.9299 Validation Accuracy: 71.1634\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 0.9494 Validation Accuracy: 68.4406\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 0.9544 Validation Accuracy: 69.0594\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.9698 Validation Accuracy: 67.6980\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 0.9585 Validation Accuracy: 67.8218\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 0.9237 Validation Accuracy: 69.6782\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 0.9603 Validation Accuracy: 67.8218\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 0.9585 Validation Accuracy: 67.9455\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.9636 Validation Accuracy: 67.4505\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 0.9632 Validation Accuracy: 68.3168\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 0.9168 Validation Accuracy: 70.6683\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 0.9573 Validation Accuracy: 67.8218\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 0.9458 Validation Accuracy: 68.6881\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.9533 Validation Accuracy: 68.0693\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 0.9661 Validation Accuracy: 68.6881\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 0.9293 Validation Accuracy: 70.9158\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 0.9474 Validation Accuracy: 68.1931\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 0.9450 Validation Accuracy: 69.4307\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.9676 Validation Accuracy: 66.8317\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 0.9626 Validation Accuracy: 68.0693\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 0.9226 Validation Accuracy: 72.0297\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 0.9484 Validation Accuracy: 68.9356\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 0.9480 Validation Accuracy: 68.8119\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.9658 Validation Accuracy: 66.7079\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 0.9614 Validation Accuracy: 67.4505\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 0.9201 Validation Accuracy: 70.7921\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 0.9488 Validation Accuracy: 68.1931\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 0.9323 Validation Accuracy: 69.9257\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.9579 Validation Accuracy: 66.8317\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 0.9542 Validation Accuracy: 69.0594\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 0.9090 Validation Accuracy: 70.2970\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 0.9431 Validation Accuracy: 69.3069\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 0.9473 Validation Accuracy: 68.3168\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.9470 Validation Accuracy: 67.3267\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 0.9612 Validation Accuracy: 69.4307\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 0.9109 Validation Accuracy: 71.0396\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 0.9441 Validation Accuracy: 68.8119\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 0.9421 Validation Accuracy: 70.2970\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.9475 Validation Accuracy: 67.2030\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 0.9621 Validation Accuracy: 69.3069\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 0.9163 Validation Accuracy: 70.0495\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 0.9314 Validation Accuracy: 69.1832\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 0.9523 Validation Accuracy: 69.1832\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.9653 Validation Accuracy: 68.0693\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 0.9638 Validation Accuracy: 67.9455\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 0.9209 Validation Accuracy: 70.2970\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 0.9314 Validation Accuracy: 68.9356\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 0.9332 Validation Accuracy: 70.4208\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.9532 Validation Accuracy: 67.6980\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 0.9510 Validation Accuracy: 69.1832\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 0.9132 Validation Accuracy: 69.3069\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 0.9355 Validation Accuracy: 69.8020\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 0.9428 Validation Accuracy: 69.5545\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.9509 Validation Accuracy: 67.3267\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 0.9639 Validation Accuracy: 68.1931\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 0.9147 Validation Accuracy: 71.1634\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 0.9345 Validation Accuracy: 69.4307\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 0.9404 Validation Accuracy: 70.1733\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.9482 Validation Accuracy: 67.9455\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 0.9549 Validation Accuracy: 68.6881\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 0.9161 Validation Accuracy: 70.1733\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 0.9317 Validation Accuracy: 69.0594\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 0.9582 Validation Accuracy: 69.6782\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.9523 Validation Accuracy: 68.3168\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 0.9525 Validation Accuracy: 68.8119\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 0.9103 Validation Accuracy: 70.5446\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 0.9384 Validation Accuracy: 69.0594\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 0.9299 Validation Accuracy: 70.2970\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.9436 Validation Accuracy: 67.6980\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 0.9588 Validation Accuracy: 68.4406\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 0.9162 Validation Accuracy: 70.0495\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 0.9301 Validation Accuracy: 70.1733\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 0.9387 Validation Accuracy: 69.9257\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.9453 Validation Accuracy: 68.4406\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 0.9436 Validation Accuracy: 68.4406\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 0.9031 Validation Accuracy: 70.6683\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 0.9265 Validation Accuracy: 69.9257\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 0.9378 Validation Accuracy: 70.0495\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.9448 Validation Accuracy: 68.8119\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 0.9606 Validation Accuracy: 67.9455\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 0.8964 Validation Accuracy: 70.5446\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 0.9414 Validation Accuracy: 69.3069\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 0.9273 Validation Accuracy: 70.0495\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.9308 Validation Accuracy: 68.9356\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 0.9737 Validation Accuracy: 70.0495\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 0.9036 Validation Accuracy: 70.7921\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 0.9342 Validation Accuracy: 68.8119\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 0.9369 Validation Accuracy: 69.6782\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.9432 Validation Accuracy: 69.1832\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 0.9467 Validation Accuracy: 68.8119\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 0.9004 Validation Accuracy: 70.9158\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 0.9266 Validation Accuracy: 69.1832\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 0.9264 Validation Accuracy: 70.4208\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.9404 Validation Accuracy: 69.8020\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 0.9463 Validation Accuracy: 68.8119\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 0.9065 Validation Accuracy: 71.5347\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 0.9353 Validation Accuracy: 69.6782\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 0.9194 Validation Accuracy: 70.5446\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.9472 Validation Accuracy: 66.9554\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 0.9508 Validation Accuracy: 69.5545\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 0.8997 Validation Accuracy: 70.7921\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 0.9359 Validation Accuracy: 69.0594\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 0.9445 Validation Accuracy: 70.0495\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.9397 Validation Accuracy: 68.3168\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 0.9438 Validation Accuracy: 69.0594\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 0.8957 Validation Accuracy: 71.5347\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 0.9241 Validation Accuracy: 69.6782\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 0.9311 Validation Accuracy: 70.9158\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.9341 Validation Accuracy: 69.1832\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 0.9374 Validation Accuracy: 69.1832\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 0.9085 Validation Accuracy: 71.4109\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 0.9188 Validation Accuracy: 69.5545\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 0.9227 Validation Accuracy: 71.4109\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.9394 Validation Accuracy: 67.8218\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 0.9409 Validation Accuracy: 69.5545\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 0.8995 Validation Accuracy: 71.6584\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 0.9231 Validation Accuracy: 69.4307\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 0.9246 Validation Accuracy: 71.0396\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.9340 Validation Accuracy: 68.4406\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 0.9431 Validation Accuracy: 70.6683\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 0.8991 Validation Accuracy: 71.9059\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 0.9251 Validation Accuracy: 69.1832\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 0.9266 Validation Accuracy: 70.9158\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.9290 Validation Accuracy: 68.0693\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 0.9484 Validation Accuracy: 70.1733\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 0.9069 Validation Accuracy: 70.4208\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 0.9230 Validation Accuracy: 70.2970\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 0.9330 Validation Accuracy: 70.9158\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.9363 Validation Accuracy: 68.6881\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 0.9336 Validation Accuracy: 67.8218\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 0.8944 Validation Accuracy: 71.7822\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 0.9222 Validation Accuracy: 69.5545\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 0.9317 Validation Accuracy: 70.7921\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.9315 Validation Accuracy: 68.6881\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 0.9399 Validation Accuracy: 68.6881\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 0.9079 Validation Accuracy: 71.4109\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 0.9155 Validation Accuracy: 69.4307\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 0.9184 Validation Accuracy: 70.1733\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.9458 Validation Accuracy: 68.5644\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 0.9425 Validation Accuracy: 69.0594\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 0.9000 Validation Accuracy: 71.6584\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 0.9183 Validation Accuracy: 69.1832\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 0.9146 Validation Accuracy: 70.9158\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.9351 Validation Accuracy: 67.6980\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 0.9332 Validation Accuracy: 69.3069\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 0.8977 Validation Accuracy: 72.4010\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 0.9253 Validation Accuracy: 69.3069\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 0.9145 Validation Accuracy: 71.1634\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.9254 Validation Accuracy: 69.0594\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 0.9371 Validation Accuracy: 69.8020\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 0.9053 Validation Accuracy: 72.1535\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 0.9096 Validation Accuracy: 69.5545\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 0.9160 Validation Accuracy: 71.6584\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.9288 Validation Accuracy: 68.9356\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 0.9203 Validation Accuracy: 69.6782\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 0.8874 Validation Accuracy: 71.5347\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 0.9216 Validation Accuracy: 69.8020\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 0.9137 Validation Accuracy: 70.5446\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.9234 Validation Accuracy: 69.5545\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 0.9317 Validation Accuracy: 69.9257\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 0.8898 Validation Accuracy: 70.7921\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 0.9080 Validation Accuracy: 69.6782\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 0.9194 Validation Accuracy: 71.6584\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.9242 Validation Accuracy: 68.8119\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 0.9226 Validation Accuracy: 70.2970\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 0.8885 Validation Accuracy: 71.4109\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 0.9068 Validation Accuracy: 71.0396\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 0.9208 Validation Accuracy: 70.2970\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.9250 Validation Accuracy: 67.5743\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 0.9336 Validation Accuracy: 69.4307\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 0.8898 Validation Accuracy: 73.1436\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 0.9140 Validation Accuracy: 70.6683\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 0.9075 Validation Accuracy: 72.0297\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.9239 Validation Accuracy: 69.0594\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 0.9269 Validation Accuracy: 70.5446\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 0.8761 Validation Accuracy: 72.1535\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 0.9069 Validation Accuracy: 70.4208\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 0.9191 Validation Accuracy: 70.4208\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.9256 Validation Accuracy: 68.0693\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 0.9221 Validation Accuracy: 70.6683\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 0.8936 Validation Accuracy: 72.2772\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 0.9029 Validation Accuracy: 69.9257\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 0.9112 Validation Accuracy: 71.0396\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.9291 Validation Accuracy: 69.4307\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 0.9192 Validation Accuracy: 69.3069\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 0.8833 Validation Accuracy: 72.1535\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 0.9041 Validation Accuracy: 70.1733\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 0.9072 Validation Accuracy: 70.2970\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.9237 Validation Accuracy: 69.3069\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 0.9225 Validation Accuracy: 69.8020\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 0.8832 Validation Accuracy: 72.0297\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 0.9081 Validation Accuracy: 69.6782\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 0.9023 Validation Accuracy: 71.2871\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.9185 Validation Accuracy: 69.3069\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 0.9316 Validation Accuracy: 70.0495\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 0.8765 Validation Accuracy: 72.0297\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 0.9105 Validation Accuracy: 70.1733\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 0.9129 Validation Accuracy: 71.2871\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.9132 Validation Accuracy: 69.3069\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 0.9299 Validation Accuracy: 70.7921\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 0.8882 Validation Accuracy: 72.7723\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 0.8983 Validation Accuracy: 69.6782\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 0.9103 Validation Accuracy: 71.5347\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.9290 Validation Accuracy: 69.3069\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 0.9168 Validation Accuracy: 69.4307\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 0.8800 Validation Accuracy: 72.2772\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 0.9106 Validation Accuracy: 70.1733\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 0.9023 Validation Accuracy: 71.4109\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.9134 Validation Accuracy: 68.5644\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 0.9171 Validation Accuracy: 70.2970\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 0.8871 Validation Accuracy: 73.2673\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 0.9027 Validation Accuracy: 69.9257\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 0.9044 Validation Accuracy: 71.0396\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./image_classification\n",
      "Testing Accuracy: 0.6098866105079651\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRERERCTS5FhEREREJPr/O6SR98KBCvkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc34195860>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_test.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for test_feature_batch, test_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: test_feature_batch, loaded_y: test_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-80% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 80%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
